"""Exemple d'utilisation du syst√®me RAG"""

import os
import sys
from pathlib import Path

# Ajouter le r√©pertoire racine au path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.config import settings
from src.rag.ingestion import DocumentIngester
from src.rag.retrieval import RetrievalSystem
from src.rag.generation import RAGGenerator
from src.rag.pipeline import RAGPipeline
from src.utils.mlflow_utils import init_mlflow, start_run, log_rag_metrics
import time


def main():
    """Exemple d'utilisation compl√®te"""
    
    # Initialiser MLflow
    init_mlflow()
    
    with start_run(run_name="example_rag_run"):
        print("üöÄ Initialisation du syst√®me RAG...")
        
        # 1. Ingestion de documents
        print("\nüìÑ √âtape 1: Ingestion de documents")
        ingester = DocumentIngester(
            chunk_size=settings.chunk_size,
            chunk_overlap=settings.chunk_overlap
        )
        
        # Exemple: ing√©rer un document (remplacer par votre chemin)
        # documents_path = "./data/sample_document.pdf"
        # if os.path.exists(documents_path):
        #     chunks = ingester.ingest(documents_path)
        #     print(f"‚úÖ {len(chunks)} chunks cr√©√©s")
        # else:
        #     print("‚ö†Ô∏è  Aucun document trouv√©, cr√©ation de chunks d'exemple")
        #     from langchain.schema import Document
        #     chunks = [
        #         Document(
        #             page_content="Python est un langage de programmation de haut niveau.",
        #             metadata={"source": "example.txt"}
        #         )
        #     ]
        
        # Pour l'exemple, cr√©ons des chunks factices
        from langchain.schema import Document
        chunks = [
            Document(
                page_content="Python est un langage de programmation de haut niveau, interpr√©t√© et orient√© objet.",
                metadata={"source": "example1.txt"}
            ),
            Document(
                page_content="LangChain est un framework pour d√©velopper des applications avec des LLM.",
                metadata={"source": "example2.txt"}
            ),
            Document(
                page_content="RAG (Retrieval-Augmented Generation) combine recherche et g√©n√©ration pour am√©liorer les r√©ponses.",
                metadata={"source": "example3.txt"}
            ),
        ]
        
        # 2. Initialisation du syst√®me de retrieval
        print("\nüîç √âtape 2: Initialisation du syst√®me de retrieval")
        retrieval_system = RetrievalSystem(
            embedding_model=settings.embedding_model,
            top_k=settings.top_k
        )
        
        # Ajouter les documents au vector store
        retrieval_system.add_documents(chunks)
        print(f"‚úÖ {len(chunks)} documents ajout√©s au vector store")
        
        # 3. Initialisation du g√©n√©rateur
        print("\nü§ñ √âtape 3: Initialisation du g√©n√©rateur RAG")
        generator = RAGGenerator(
            llm_model=settings.llm_model,
            temperature=settings.temperature,
            max_tokens=settings.max_tokens
        )
        
        # 4. Cr√©ation du pipeline complet
        print("\nüîó √âtape 4: Cr√©ation du pipeline RAG")
        pipeline = RAGPipeline(retrieval_system, generator)
        
        # 5. Ex√©cution d'une requ√™te
        print("\nüí¨ √âtape 5: Ex√©cution d'une requ√™te")
        question = "Qu'est-ce que Python?"
        
        start_time = time.time()
        result = pipeline.run(question=question)
        latency = time.time() - start_time
        
        print(f"\n‚ùì Question: {result['question']}")
        print(f"‚úÖ R√©ponse: {result['answer']}")
        print(f"üìö Sources: {len(result['sources'])} documents")
        print(f"‚è±Ô∏è  Latence: {latency:.2f}s")
        
        # Logging des m√©triques
        log_rag_metrics(
            question=question,
            answer=result['answer'],
            retrieved_docs_count=len(result['sources']),
            answer_length=len(result['answer']),
            model=result['model'],
            latency=latency
        )
        
        # 6. Exemple de recherche
        print("\nüîé √âtape 6: Recherche dans le vector store")
        search_results = retrieval_system.similarity_search("LangChain", k=2)
        print(f"‚úÖ {len(search_results)} r√©sultats trouv√©s")
        for i, doc in enumerate(search_results, 1):
            print(f"  {i}. {doc.page_content[:100]}...")
        
        print("\n‚ú® Exemple termin√© avec succ√®s!")


if __name__ == "__main__":
    # V√©rifier que les cl√©s API sont configur√©es
    if not settings.openai_api_key:
        print("‚ö†Ô∏è  Attention: OPENAI_API_KEY n'est pas configur√©e")
        print("   Configurez-la dans le fichier .env ou comme variable d'environnement")
        sys.exit(1)
    
    main()



